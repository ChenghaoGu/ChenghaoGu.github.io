<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>IGen</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://[YOUR_GITHUB_USERNAME].github.io/igen/img/teaser.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://[YOUR_GITHUB_USERNAME].github.io/igen/"/>
    <meta property="og:title" content="IGen: Scalable Data Generation for Robot Learning from Open-World Images" />
    <meta property="og:description" content="[PROJECT_DESCRIPTION_PLACEHOLDER]" />

    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22  viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¤–</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css ">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css ">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css ">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js "></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js "></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js "></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js "></script>
    
    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center" style="font-size: 3em;">
                <b style="color: #3B6978;">IGen</b>: 
                <span style="color: #84A9AC;">Scalable Data Generation for Robot Learning from Open-World Images</span>
                </br> 
            </h2>
        </div>
        <br>
        <div class="row">
            <div class="col-md-12 text-center">
                <div class="publication-authors" style="font-size: 20px;">
                    <div class="author-row">
                        <span class="author-block">
                            Chenghao Gu<sup>1*</sup>,
                        </span>
                        <span class="author-block">
                            Haolan Kang<sup>2*</sup>,
                        </span>
                        <span class="author-block">
                            Junchao Lin<sup>3*</sup>,
                        </span>
                        <span class="author-block">
                            Jinghe Wang<sup>1</sup>,
                        </span>
                        <span class="author-block">
                            Duo Wu<sup>1</sup>,
                        </span>
                        <span class="author-block">
                            Shuzhao Xie<sup>1</sup>,
                        </span>
                    </div>
                    <div class="author-row">
                        <span class="author-block">
                            Fanding Huang<sup>1</sup>,
                        </span>
                        <span class="author-block">
                            Junchen Ge<sup>1</sup>,
                        </span>
                        <span class="author-block">
                            Ziyang Gong<sup>4</sup>,
                        </span>
                        <span class="author-block">
                            Letian Li<sup>1</sup>,
                        </span>
                        <span class="author-block">
                            Hongying Zheng<sup>5</sup>,
                        </span>
                        <span class="author-block">
                            Changwei Lv<sup>5</sup>,
                        </span>
                        <span class="author-block">
                            Zhi Wang<sup>1</sup>
                        </span>
                    </div>
                </div>
                <br>
                <div class="is-size-2 publication-authors" style="font-size: 15px;">
                    <span class="author-block"><sup>1</sup>Tsinghua University. </span><br>
                    <span class="author-block"><sup>2</sup>HKU. </span>
                    <span class="author-block"><sup>3</sup>Beijing University of Chemical Technology. </span><br>
                    <span class="author-block"><sup>4</sup>Shanghai Jiao Tong University. </span>
                    <span class="author-block"><sup>5</sup>Shenzhen University of Infomation Technology. </span><br>
                </div>    
                <div class="qually" style="font-size: 12px; color: #888888;">
                    <span class="author-block"><sup>*</sup>indicates equal contributions. </span>
                </div>
                <br>
            </div>
        </div>
        


        <div class="row">
                <div class="col-md-6 col-md-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="[arXiv_PDF_LINK]">
                            <image src="img/arXiv_icon.png" height="40px">
                                <h4><strong>arXiv</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="[CODE_GITHUB_LINK]">
                            <image src="img/github.png" height="40px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="[VIDEO_LINK]">
                            <image src="img/video_icon.png" height="40px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <video id="v0" width="100%" autoplay loop muted controls>
                    <source src="video/1_IGen_Overview.mp4" type="video/mp4" />
                  </video>
                <p class="text-justify">
                IGen is a scalable data generation system for robot learning from open-world images. Instead of relying on labor-intensive teleoperation, IGen takes diverse images from the real world and automatically synthesizes robot trajectories for various tasks.
                With these generated data, we can train robot policies that transfer to real robots and perform effective manipulation in the real world â€” without any human teleoperation data.
                </p>
            </div>
        </div> 
          
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    The rise of generalist robotic policies has created an exponential demand for large-scale training data. However, on-robot data collection is labor-intensive and often limited to specific environments. In contrast, open-world images capture a vast diversity of real-world scenes that naturally align with robotic manipulation tasks, offering a promising avenue for low-cost, large-scale robot data acquisition. Despite this potential, the lack of associated robot actions hinders the practical use of open-world images for robot learning, leaving this rich visual resource largely unexploited. To bridge this gap, we propose IGen, a framework that scalably generates realistic visual observations and executable actions from open-world images. IGen first converts unstructured 2D pixels into structured 3D scene representations suitable for scene understanding and manipulation. It then leverages the reasoning capabilities of vision-language models to transform scene-specific task instructions into high-level plans and generate low-level actions as SE(3) end-effector pose sequences.  From these poses, it synthesizes dynamic scene evolution and renders temporally coherent visual observations. Experiments validate the high quality of visuomotor data generated by IGen, and show that policies trained solely on IGen-synthesized data achieve performance comparable to those trained on real-world data. This highlights the potential of IGen to support scalable data generation from open-world images for generalist robotic policy training. Code for IGen will be made publicly available.
                </p>
            </div>
        </div>
  
<br>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Method
                </h3>
                <img width="100%" src="./img/IGen_Method.png"/>
                <p class="text-justify">
                    Given an open-world image and a task description, IGen first reconstructs the environment and objects as point clouds via Foundation Vision Models. After spatial keypoint extraction, Vision-Language-Model maps the task description to high-level plans and low-level control commands. During the robotâ€™s execution in simulation, a virtual depth camera captures the motion point cloud sequences. The resulting end-effector pose trajectory is used to synthesize dynamic point-cloud sequences, which are then rendered frame-by-frame into visual observations of the manipulation. The final output consists of the generated robot actions and the visual observations. 
                </p>
            </div>
        </div>
<br>
<!-- 
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Main Results
                </h3>
                <img width="100%" src="./img/main_results.png"/>
                <p class="text-justify">
                    [RESULTS_DESCRIPTION_PLACEHOLDER]
                </p>
            </div>
        </div> 
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Visualization
                </h3>
                <p class="text-justify">
                    [VISUALIZATION_DESCRIPTION_PLACEHOLDER]
                </p>

                <img width="100%" src="./img/vis/example1.png"/>
                <p class="text-justify" style="text-align: center;">
                    <em>[EXAMPLE1_TITLE]</em>
                </p>

                <img width="100%" src="./img/vis/example2.png"/>
                <p class="text-justify" style="text-align: center;">
                    <em>[EXAMPLE2_TITLE]</em>
                </p>
            </div>
        </div>  -->

        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{[CITATION_KEY],
    title={IGen: Scalable Data Generation for Robot Learning from Open-World Images},
    author={[AUTHOR_NAMES]},
    booktitle={[CONFERENCE_NAME]},
    year={[YEAR]}
}
                </textarea>
                </div>
            </div>
        </div>

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgments
                </h3>
                <p class="text-justify">
                    [ACKNOWLEDGMENTS_TEXT_PLACEHOLDER]
                </p>
            </div>
        </div> -->
    </div>
</body>
</html>